{"pages":[{"title":"about","text":"I’m interested in time series data and now mainly focusing on electrocardiogram related research. I am working at VUNO since 2018 as a deep learning researcher. I hope my research will contribute to saving people’s lives.","link":"/about/index.html"}],"posts":[{"title":"Jinwoo's Blog","text":"My blog begins here!","link":"/2020/02/16/hello-world/"},{"title":"Resnet(1) - Deep Residual Learning for Image Recognition","text":"Resnet 모델을 제안한 Kaiming He가 쓴 논문은 총 두 편이 있습니다. Deep residual learning for image recognition. CVPR. (2016) Identity mappings in deep residual networks. ECCV. (2016) 이 가운데 첫 번째 논문 ‘Deep residual learning for image recognition’에 대하여 다루어보고자 합니다. Resnet은 2015년 ILSVRC에서 우승을 차지한 모델입니다. 아래 그림과 같이, 이전 대회에서 우승한 GoogleNet의 22개 층보다 약 7배 더 많은 152개 층을 쌓아서 학습하는데 성공하였고 3.57% Error rate으로 Imagenet 대회 우승을 차지하였습니다. 본 논문에서는 이와 같은 성과를 가능하게한 ‘Residual learning’ 에 대하여 이론적으로 정의하고 실험적으로 증명합니다. Degradation problemResidual learning이 나오게 된 배경은 degradation problem에서부터 시작합니다. 일반적으로 모델의 depth가 깊어지면 과잉 학습으로 인하여 모델이 학습 데이터에 overfit이 된다고 알려져 있습니다. 그러나 depth가 더욱 깊어지면 그와 반대되는 현상이 발생합니다. 아래 그림과 같이, 층을 많이 쌓은 모델이 적게 쌓은 모델보다 학습 에러가 더 높게 나타나면서 underfit 되는 현상이 발생합니다. 즉, 모델이 학습을 제대로 해내지 못합니다. Training error (left) and test error (right) on CIFAR-10 with 20-layer and 56-layer “plain” networks. The deeper network has higher training error, and thus test error. 저자는 이와 같은 현상을 Degradation problem이라고 정의하였고, 이를 하나의 실험을 통해서 보여줍니다. 실험은 다음과 같습니다. 우선 층 수가 적은 모델(왼쪽)을 학습한 뒤, 그 모델을 그대로 복사한 모델을 생성합니다. 복사한 모델의 중간, 중간에는 입력 값을 그대로 출력하는 identity mapping을 수행하는 층을 추가하여 전체 층이 더 깊은 모델 (오른쪽)로 만들어 줍니다. 전체 층 수는 늘었지만, 입력을 그대로 출력하는 층들을 추가한 것입니다. 따라서 우리는 이 모델을 학습하면 최소한 이전 모델보다 성능이 나쁘지는 않을 것이라고 기대할 수 있습니다. 그러나 실제 실험결과는 우리의 기대와 다르게 학습 에러가 더 높게 나타나면서 앞서 정의한 degradation problem이 발생합니다. 저자는 실험을 통해서 본 바와 같이 깊은 모델을 학습하는 것은 매우 어렵다고 말하면서, 이를 해결하는 방법으로 Identity mapping을 이용한 Residual learning을 제안합니다. Residual Learning‘Residual’이란 우리말로 ‘잔차’라고 표현하며, 잔차란 예측값과 실제값의 차이로 생기는 값을 의미합니다. 즉 딥러닝에서 에러, 손실 값이라고 말할 수 있습니다.실제 값 10을 우리가 8로 예측했다면 잔차는 2가 됩니다.따라서 아래 그림과 같이, 실제 값(Actual)은 우리의 예측 값(x)과 잔차(residual)의 합으로 표현할 수 있습니다. 우리의 목표는 예측값을 실제값과 같게 만드는 것입니다.따라서 예측값과 실제값의 차이, 즉 잔차를 줄이는 방향으로 학습을 하면 되는 것입니다.실제값 10을 8로 예측했다면 잔차 2에 대해서만 학습하면 되는 것이고, 10으로 예측 했다면 우리는 잔차를 학습할 필요가 없습니다. 이렇게 잔차를 학습하는 학습 방식을 Residual Learning 이라고 합니다. 기존의 딥러닝에서는 입력값(x)을 실제값(y)로 맵핑하는 함수 H(x)를 찾는 것으로H(x) - y를 최소화 하는 방향으로 학습을 진행하였습니다. 반면 Resnet에서는 H(x) - x를 학습하는 것으로 재정의한 것입니다. 입력과 출력의 잔차를 F(x) = H(x) - x라고 정의하고 이 F(x)를 찾는 모델을 학습하는 것입니다. 결과적으로 출력 H(x)는 아래와 같이 F(x) + x가 됩니다. Identity mapping with shortcut (계속 …) References resnet_cam 데이터 사이언스 스쿨","link":"/2020/02/23/Resnet-1-Deep-Residual-Learning-for-Image-Recognition/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","link":"/tags/deeplearning/"},{"name":"cnn","slug":"cnn","link":"/tags/cnn/"}],"categories":[{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"},{"name":"Model","slug":"Deep-Learning/Model","link":"/categories/Deep-Learning/Model/"}]}